#!/usr/bin/python3
# encoding =UTF-8

import os
import sys
import random
import numpy as np
import scipy as sp
import argparse
import pyfaidx
import gzip
import pickle as pickle
from SequenceContainer import ReadContainer


# Parsing all input arguments

parser = argparse.ArgumentParser(description='RNASeqDesigner Version 1')
# parser.add_argument('-h', type=int, required=True, default=100, metavar='<i>', help='help function')
parser.add_argument('-r',           type=int,        required=True,      default=101,        metavar='<int>',           help='Read length')
parser.add_argument('-n',           type=int,        required=True,      default=10000,      metavar='<int>',           help='Number of reads to simulate')
parser.add_argument('-f',           type=str,        required=True,                          metavar='<str>',           help='Reference transcriptome file')
parser.add_argument('-s',           type=int,        required=False,     default=1223,       metavar='<int>',           help='Random seed for reproducibility')
parser.add_argument('-o',           type=str,        required=True,                          metavar='<str>',           help='Output prefix')
parser.add_argument('-seqmodel',    type=str,        required=True,                          metavar='<str>',           help='Sequencing_model')
parser.add_argument('-countModel',  type=str,        required=False,                         metavar='<str>',           help='transcript expression count model')
parser.add_argument('-er',          type=float,      required=False,     default=-1,         metavar='<int>',           help='Error rate')
parser.add_argument('-FL',          nargs=2,         type=int, required=False,     default=(250, 25),   metavar=('<int>','<int>'), help='Fragment length distribution parameters')
parser.add_argument('-SE',          action='store_true', required=False,                                                help='Flag for producing single-end reads ')
parser.add_argument('-PE',          action='store_true',        required=False,                                         help='Flag for producing paired-end reads')

args = parser.parse_args()

(fragment_size, fragment_std) = args.FL
ref = args.f
readlen = args.r
readtot = args.n
SEED = args.s
output = args.o
sqmodel = args.seqmodel
countModel = args.countModel
SE = args.SE
PE = args.PE
SE_RATE = args.er

paired = False
single = False
counts = False
profile = False

if fragment_size != None and fragment_std != None:
    paired = True

elif PE != None:
    paired = True

if countModel != None:
    profile = True

if SE != None:
    single = True
np.random.seed(SEED)
NB_model = np.round(sp.random.negative_binomial(n=1, p=0.1, size=10000))
NB_model = [1 if x == 0 else x for x in NB_model]

pyfaidx.Faidx(ref)

# Search current directory for index file and read into programme
indexFile = ''
for file in os.listdir('.'):
    if file.endswith('.fai'):
        indexFile = (os.path.join('.', file))

SE_CLASS = ReadContainer(readlen, sqmodel, SE_RATE)

# Process and read in all models
if countModel != None:

    tissue_profile = open(countModel, 'rb')
    transcript_ids = pickle.load(tissue_profile, encoding='latin1')
    count_table = pickle.load(tissue_profile, encoding='latin1')


def normdist(low, high=None, size=None):
    """
    Description
    This function generates random numbers according to a specified distribution
    Parameters
    low (int): The lowest value to sample
    high (int) the upper bound for sampling
    Return: Returns a vector of numbers drawn from a user-specified distribution
    """
    np.random.seed(SEED)
    counts = np.round(np.random.uniform(low=0, high=high, size=size))

    return counts


def parseIndexRef(indexFile):
    """
    Description:
    Read in sequence data from reference index FASTA file returns a list of transcript IDs
    offset, seqLen, position
    Parameters
     - indexFile (str): The index file generated by the program, written to the current directory
    Return: The function returns a list of tuples containing the transcript id, start and end offset of the transcript sequence
    """
    ref_inds = []
    fai = open(indexFile, 'r')
    for line in fai:
        splt = line[:-1].split('\t')
        header = '@' + splt[0]
        seqLen = int(splt[1])
        offset = int(splt[2])
        lineLn = int(splt[3])
        nLines = seqLen / lineLn

        if seqLen % lineLn != 0:
            nLines += 1
        ref_inds.append((header, offset, offset+seqLen+nLines))
    fai.close()
    return ref_inds


def samplingtranscripts(ids):
    """"
    Description: This function randomly sample from all reference transcripts
    Parameters: ids (list of tuples) It takes as input all reference transcripts offsets
    Returns: This function returns a subset of transcript ids to be sampled from
    """
    random.seed(SEED)
    numreads = readtot
    sampledtranscripts = random.sample(ids, numreads)

    return sampledtranscripts

def getseq(key, start=1, end=None):
    """
    Description
    Get a sequence by key coordinates are 1-based and end is inclusive
    Parameters:
        key:
        start:
        end:
    Returns:
    """

    if end != None and end < start:
        return ""
    start -= 1
    seek = start

    # if seek is past sequence then return empty sequence
    if seek >= end:
        return ""

    # seek to beginning
    infile = open(ref, 'r')
    infile.seek(seek)

    # read until end of sequence
    header = ''
    seq = []
    if end == None:
        lenNeeded = util.INF
    else:
        lenNeeded = end - start

    len2 = 0
    while len2 < lenNeeded:
        line = infile.readline()
        if line.startswith(">") or len(line) == 0:
            break
        seq.append(header + line.rstrip())
        len2 += len(seq[-1])
        if len2 > lenNeeded:
            seq[-1] = seq[-1][:-int(len2 - lenNeeded)]
            break
    seq = "".join(seq)
    return seq


def processTransIDs(ids):
    """"
    Description:
    This function take as input a list of transcript ids and converts it to a dictionary
    Parameters:
        ids (list of tuples): List of transcript ids
    Returns: The function returns a dictionary of transcript id as key and start and end position as value
    """

    Transseq = []
    header = []
    transcriptID = {i: [j, k] for i, j, k in ids}
    ID = transcriptID.keys()
    for k in ID:
        header.append(k)
    pos = transcriptID.values()
    for i in pos:
        start = i[0]
        end = i[1]
        seq = getseq(ID, start, end)
        Transseq.append(seq)
    # print(Transseq)
    new_dict = {k: v for k, v in zip(header, Transseq)}
    return new_dict

def GenerateRead(seq, readLen, numstarts):
    """
    Description:
    This function truncates transcript sequences by a specified read length.
    Parameters:
    :param seq: Transcript sequence randomly sampled from the input reference transcriptome file
    :param readLen: The user-specified read length

    :return: The function returns a list of all truncated sequences
    """

    seqLen = len(seq)
    endmax = seqLen - (readLen + 1)
    startpos = list(set(np.sort(np.round(np.random.uniform(low=1, high=endmax, size=numstarts))).tolist()))

    return [startpos, seqLen, endmax]


def sample_qualscore(sequencingModel):
    (myQual, myErrors) = SE_CLASS.getSequencingErrors(sequencingModel)
    return myQual

random.seed(1234)

def main():

    global quality_strings

    ref_transcript_ids = parseIndexRef(indexFile)
    if SE == True:

        sample_trans_ids = []
        COUNTS = []
        quality_strings = []
        reads = []
        ids = []

        if single == True and countModel == None:
            counts = np.random.choice(NB_model, size=readtot, replace=True).tolist()

            samptransids = random.choices(ref_transcript_ids, k=len(counts))

            sample_trans_ids.append(samptransids)
            COUNTS.append(counts)

        elif single == True and countModel != None:
            samptransids = random.choices(ref_transcript_ids, k=len(count_table))
            sample_trans_ids.append(samptransids)

        for i, j in zip(sample_trans_ids, COUNTS[0]):
            p = processTransIDs(i)
            for id, seq in p.items():
                readinfo = GenerateRead(seq, readlen, j)
                startpos = readinfo[0]

                sp = random.choices(startpos, k=j)
                endpos = [i + readlen for i in sp]
                endmax = readinfo[2]
                seqlen = readinfo[1]
                for k, l in zip(sp, endpos):
                    read = seq[int(k):int(l)]
                    qdata = sample_qualscore(sequencingModel=sqmodel)
                    ids.append(id)
                    reads.append(read)
                    quality_strings.append(qdata)

        with open('read1.fastq', 'w') as handle:
            for id, read, q in zip(ids, reads, quality_strings):
                handle.write('{}\n{}\n+\n{}\n'.format(id, read, q))
    if PE == True:

        fragments = []
        ids = []
        R1 = []
        R2 = []
        RFS = []
        # RFS_final = []
        COUNTS_P = []
        quality_strings = []

        if paired and countModel == None:
            counts = np.random.choice(NB_model, size=readtot, replace=True).tolist()
            COUNTS_P.append(counts)
        elif paired and countModel != None:
            COUNTS_P.append(count_table)
        FS = np.random.normal(fragment_size, fragment_std, 100).astype(int).tolist()
        for i in COUNTS_P[0]:
            randomFS = random.choices(FS, k=i)
            RFS.append(randomFS)
        samptransids = random.choices(ref_transcript_ids, k=len(COUNTS_P[0]))

        RFS_final = []
        for list in RFS:
            for number in list:
                RFS_final.append(number)

        for j, f in zip(samptransids, RFS_final):

            p = processTransIDs([j])
            for id, seq in p.items():
                readinfo = GenerateRead(seq, f, len(RFS_final))
                startpos = readinfo[0]
                endpos = [i + f for i in startpos]

                for k, l in zip(startpos, endpos):
                    frag = seq[int(k):int(l)]
                    qdata = sample_qualscore(sequencingModel=sqmodel)
                    ids.append(id)
                    fragments.append(frag)
                    quality_strings.append(qdata)

        for i in fragments:
            read1 = i[:readlen]
            read2 = i[-readlen:]
            R1.append(read1)
            R2.append(read2)

        with open('read1.fastq', 'w') as handle:
            for id, read, q in zip(ids, R1, quality_strings):
                handle.write('{}\n{}\n+\n{}\n'.format(id, read, q))

        with open('read2.fastq', 'w') as handle:
            for id, read, q in zip(ids, R2, quality_strings):
                handle.write('{}\n{}\n+\n{}\n'.format(id, read, q))


if __name__ == '__main__':
    main()













































































# f = []
# ids = []
# R1 = []
# R2 = []
# seqstring = []
# qstrings = []
# outputR1 = 'read1.fastq'
# outputR2 = 'read2.fastq'
#
# counts = processTransTable(CountTable)
# dcounts = counts[1:5]
# transcriptID = parseIndexRef(indexFile)
# transID = [random.choices(transcriptID, k=len(dcounts))]
#
# for i, r in zip(transID, dcounts):
#     # print(i, r)
#     p = processTransIDs(i)
#     for key, value in p.items():
#         ids.append(key)
#         seqstring.append(value)
#
# for i, w in zip(seqstring, dcounts):
#     finalseq = GenerateRead(i, readlen, w)
#     f.append(finalseq)
# rinfo = []
# for i in seqstring:
#     readinfo = GenerateRead(i, readlen, 250)
#     rinfo.append(readinfo)
#
# print(type(rinfo[0][1]))
# startpos = rinfo[0][0]
# endpos = rinfo[0][1]
# ep = random.choices(endpos, k=5)
# print(ep)
# print(len(seqstring))
# print(f[1:3])
# print(seqstring)
# print(len(ids))

# print(len(ids))
# print(dcounts)
    # for i in f:
    #     qdata = sample_qualscore(sequencingModel=sqmodel)
    #     qstrings.append(qdata)
    #
    # reads = zip(ids, f, qstrings)
    # with open(output, 'w') as handle:
    #     for k, v, i in reads:
    #         handle.write('{}\n{}\n+\n{}\n'.format(k, v, i))
    # print('Writing output to file')
    
    
# print(f[1:3])

#
# A_counts = np.array(counts)
# LCounts = np.log(A_counts)
#
# plt.hist(LCounts)
# plt.show()


# p = processTransIDs(transID)

#
#
# for i in f:
#     g = i.tolist()
#     seqLen = len(g)
#     endmax = seqLen - (readlen + 1)
#     R1startpos = 1
#     R1endpos = readlen + 1
#     R2startpos = -readlen
#     R2endpos = -1
#     startpos = np.sort(normdist(low=0, high=endmax))
#     # print(startpos)
#     endpos = [i + readlen for i in startpos]
#
#     read1 = np.array(g[R1startpos:R1endpos])
#     read2 = g[R2startpos:R2endpos]
#     R1.append(read1)
#     R2.append(read2)
#     for r in R1:
#         qdata = sample_qualscore(sequencingModel=sqmodel)
#         qstrings.append(qdata)
#
#
#     fR1 = zip(ids, R1, qstrings)
#     fR2 = zip(ids, R2, qstrings)
#     with open(outputR1, 'w') as handle:
#         for k, v, i in fR1:
#             handle.write('{}\n{}\n+\n{}\n'.format(k, v, i))
#     with open(outputR2, 'w') as handle:
#         for k, v, i in fR2:
#             handle.write('{}\n{}\n+\n{}\n'.format(k, v, i))
#
# print('Writing output to file')
#
#
#
#
# # print(qstrings)
# # def main():
# #
# #     f = []
# #     ids = []
# #     qstrings = []
# #
# #     transcriptIDs = parseIndexRef(indexFile)
# #     print('Converting reference to index file')
# #     s = samplingtranscripts(transcriptIDs)
# #     print(len(s))
# #     print('Random sampling from transcript file')
#
#     p = processTransIDs(s)
#
#     for key, value in p.items():
#         finalseq = GenerateRead(value, readlen)
#         f.append(finalseq)
#         ids.append(key)
#
#     for i in f:
#         qdata = sample_qualscore(sequencingModel=sqmodel)
#         qstrings.append(qdata)
#
#     reads = zip(ids, f, qstrings)
#
#     with open(output, 'w') as handle:
#         for k, v, i in reads:
#             handle.write('{}\n{}\n+\n{}\n'.format(k, v, i))
#     print('Writing output to file')
#
#





















